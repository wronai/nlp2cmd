{
  "timestamp": "2026-01-27T11:34:29.813392",
  "summary": {
    "average_single_command_latency_ms": 32.9785120666808,
    "average_sequential_latency_ms": 27.129445100066125,
    "average_throughput_commands_per_second": 36.907173105939066,
    "fastest_adapter_single": "docker",
    "fastest_adapter_sequential": "docker",
    "highest_throughput_adapter": "docker"
  },
  "adapters": {
    "shell": {
      "single_command": {
        "command": "Find all Python files in the current directory",
        "average_latency_ms": 33.73724079992826,
        "all_latencies": [
          48.92134999954578,
          39.83298399998603,
          26.035826999759593,
          26.44046299974434,
          27.45558000060555
        ]
      },
      "sequential": {
        "total_time_ms": 287.2409850006079,
        "individual_latencies": [
          26.276380000126665,
          29.946593000204302,
          31.020213000374497,
          30.138038000586675,
          26.49130600002536,
          30.105973999525304,
          26.853910000681935,
          30.337686000166286,
          28.889363999951456,
          26.988200999767287
        ],
        "average_latency_ms": 28.704766500140977,
        "min_latency_ms": 26.276380000126665,
        "max_latency_ms": 31.020213000374497,
        "median_latency_ms": 29.41797850007788,
        "std_deviation_ms": 1.8496339821167593
      },
      "throughput_commands_per_second": 34.813973361005
    },
    "sql": {
      "single_command": {
        "command": "Select all users from the database",
        "average_latency_ms": 38.93415620023006,
        "all_latencies": [
          81.19766300023912,
          29.38901499965141,
          28.35911800048052,
          27.42754300015804,
          28.29744200062123
        ]
      },
      "sequential": {
        "total_time_ms": 269.16342500044266,
        "individual_latencies": [
          31.936822000716347,
          28.412568000021565,
          26.183062000200152,
          26.960172999679344,
          25.904631999765115,
          26.406874000713287,
          27.52252300069813,
          26.294745999621227,
          24.92052199977479,
          24.42844300003344
        ],
        "average_latency_ms": 26.89703650012234,
        "min_latency_ms": 24.42844300003344,
        "max_latency_ms": 31.936822000716347,
        "median_latency_ms": 26.350810000167257,
        "std_deviation_ms": 2.1131781236279514
      },
      "throughput_commands_per_second": 37.1521502224292
    },
    "docker": {
      "single_command": {
        "command": "List all running containers",
        "average_latency_ms": 26.264139199884085,
        "all_latencies": [
          26.850107999962347,
          26.641344999916328,
          27.10876199944323,
          24.77479899971513,
          25.945682000383385
        ]
      },
      "sequential": {
        "total_time_ms": 258.02858700080833,
        "individual_latencies": [
          25.56128899959731,
          25.105283999437233,
          25.568328000190377,
          25.687377000394918,
          25.528046000545146,
          25.378292999448604,
          26.54702299969358,
          26.833486999748857,
          26.7394049997165,
          24.916791000578087
        ],
        "average_latency_ms": 25.78653229993506,
        "min_latency_ms": 24.916791000578087,
        "max_latency_ms": 26.833486999748857,
        "median_latency_ms": 25.564808499893843,
        "std_deviation_ms": 0.6786121238923742
      },
      "throughput_commands_per_second": 38.755395734383
    }
  },
  "markdown_report": "# NLP2CMD Performance Benchmark Report\n\n*Generated on: 2026-01-27T11:34:29.813392*\n\n## Executive Summary\n\nThis report analyzes the performance characteristics of NLP2CMD when processing single versus sequential commands, with focus on identifying bottlenecks and optimizing for energy efficiency and throughput based on thermodynamic principles.\n\n### Key Findings\n\n- **Total Time Saved (10 commands)**: 174.9ms (17.7% efficiency gain)\n- **Average Throughput**: 36.91 commands/sec\n- **Best Performing Adapter**: docker (sequential processing)\n\n## Performance Comparison Table\n\n| Adapter | Single Command (ms) | Sequential Avg (ms) | Total Time (10 cmd) | Throughput (cmd/s) | Time Saved (ms) |\n|---------|-------------------|-------------------|-------------------|------------------|----------------|\n| SHELL | 33.7 | 28.7 | 287.2 | 34.81 | 50.1 |\n| SQL | 38.9 | 26.9 | 269.2 | 37.15 | 120.2 |\n| DOCKER | 26.3 | 25.8 | 258.0 | 38.76 | 4.6 |\n\n## Thermodynamic Performance Analysis\n\n### Energy Efficiency Metrics\n\nBased on thermodynamic principles, we analyze the system's energy consumption patterns:\n\n1. **Initialization Energy (E_init)**: Energy required to cold-start the NLP2CMD system\n2. **Processing Energy (E_proc)**: Energy per command during steady state\n3. **Idle Energy (E_idle)**: Energy consumption between commands\n\n#### Calculated Energy Savings\n\n| Adapter | E_init (ms) | E_proc per cmd (ms) | Total Energy (10 cmd) | Energy Saved |\n|---------|------------|-------------------|---------------------|-------------|\n| SHELL | 50.1 | 28.7 | 287.2 | 40.1 |\n| SQL | 120.2 | 26.9 | 269.2 | 96.1 |\n| DOCKER | 4.6 | 25.8 | 258.0 | 3.7 |\n\n## Bottleneck Analysis\n\n### Identified Performance Bottlenecks\n\n1. **Model Loading Time**\n   - Average initialization overhead: 58.3ms\n   - Impact: Most significant for single command execution\n   - Recommendation: Implement model pre-loading and connection pooling\n\n2. **Memory Allocation Patterns**\n   - Sequential processing shows 17.7% efficiency gain\n   - Indicates memory reuse benefits\n   - Recommendation: Maintain persistent memory pools\n\n3. **Adapter Initialization**\n   - Fastest adapter: docker (25.8ms avg)\n   - Slowest adapter: shell\n   - Recommendation: Optimize adapter factory patterns\n\n### Optimization Strategies\n\n#### 1. Thermodynamic Minimization (Energy Focus)\n- Keep system at \"thermal equilibrium\" (warm state)\n- Minimize phase transitions (cold starts)\n- Use lazy loading with persistent caches\n- **Expected improvement**: 30-40% energy reduction\n\n#### 2. Entropy Reduction (Speed Focus)\n- Pre-compile command patterns\n- Use deterministic routing\n- Implement command batching\n- **Expected improvement**: 20-25% speed increase\n\n#### 3. Free Energy Optimization (Balance)\n- Balance between initialization cost and processing speed\n- Adaptive timeout based on command complexity\n- Dynamic resource allocation\n- **Expected improvement**: 25-35% overall efficiency\n\n## Processing Mode Comparison\n\n### Mode 1: Individual Command Processing\n```\nTotal Time = n \u00d7 (T_init + T_proc)\nEnergy = n \u00d7 (E_init + E_proc)\n```\n- **Pros**: Isolated execution, no state pollution\n- **Cons**: High initialization overhead\n- **Best for**: Sporadic, low-frequency usage\n\n### Mode 2: Sequential Batch Processing\n```\nTotal Time = T_init + n \u00d7 T_proc\nEnergy = E_init + n \u00d7 E_proc\n```\n- **Pros**: Amortized initialization cost\n- **Cons**: State persistence required\n- **Best for**: High-frequency, batch operations\n\n### Mode 3: Thermodynamic Hybrid (Recommended)\n```\nTotal Time = T_init + n \u00d7 T_proc - T_adaptive\nEnergy = E_init + n \u00d7 E_proc - E_reclaimed\n```\n- **Pros**: Adaptive optimization, energy reclamation\n- **Cons**: Complex implementation\n- **Best for**: Production systems with variable load\n\n## Recommendations\n\n### Immediate Actions (Week 1)\n1. Implement connection pooling for adapters\n2. Add model pre-loading option\n3. Create batch processing API\n\n### Short Term (Month 1)\n1. Develop thermodynamic scheduler\n2. Implement energy monitoring\n3. Add adaptive timeout mechanisms\n\n### Long Term (Quarter 1)\n1. Full thermodynamic optimization engine\n2. Predictive pre-loading based on usage patterns\n3. Energy-efficient resource orchestration\n\n## Conclusion\n\nThe benchmark demonstrates significant efficiency gains (17.7%) when using sequential processing. The thermodynamic approach to optimization provides a framework for balancing energy consumption with processing speed, leading to more sustainable and efficient NLP2CMD deployments.\n\n---\n*Report generated by NLP2CMD Benchmark Tool v1.0*\n"
}