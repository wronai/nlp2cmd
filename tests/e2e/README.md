# E2E Tests for NLP2CMD Service Mode

This directory contains end-to-end tests for the NLP2CMD service mode implementation.

## ðŸ“‹ Test Coverage

### âœ… Core Functionality Tests
- **Service Startup** - Verify service starts correctly
- **Health Check** - Test `/health` endpoint
- **Service Info** - Test root `/` endpoint with configuration
- **Basic Query Processing** - Test natural language to command conversion
- **Polish Language Support** - Test Polish queries (znajdÅº, uruchom, pokaÅ¼)
- **Query with Explanation** - Test explanation functionality
- **Configuration Management** - Test config get/update endpoints
- **Error Handling** - Test invalid requests and error responses
- **Concurrent Requests** - Test multiple simultaneous requests

### ðŸ”§ Technical Tests
- **Service Lifecycle** - Start/stop/restart scenarios
- **Configuration Persistence** - .env file management
- **API Validation** - Pydantic model validation
- **Performance** - Response time and concurrency
- **Integration** - Full workflow testing

## ðŸš€ Running Tests

### Quick Start (Simple Runner)

```bash
# Run all E2E tests with default settings
python3 run_e2e_tests.py

# Run with custom host/port
python3 run_e2e_tests.py --host 127.0.0.1 --port 8002

# Verbose output
python3 run_e2e_tests.py --verbose
```

### With pytest

```bash
# Run all E2E tests
pytest tests/e2e/ -v

# Run only service tests
pytest tests/e2e/ -m service

# Run with coverage
pytest tests/e2e/ --cov=nlp2cmd.service --cov-report=html

# Run specific test file
pytest tests/e2e/test_service_e2e.py -v
```

## ðŸ“ File Structure

```
tests/e2e/
â”œâ”€â”€ conftest.py              # Pytest configuration and fixtures
â”œâ”€â”€ test_service_e2e.py      # Main E2E test suite
â””â”€â”€ README.md               # This file

run_e2e_tests.py             # Simple test runner (no pytest needed)
pytest.ini                  # Pytest configuration
```

## ðŸ§ª Test Categories

### Service Tests (`@pytest.mark.service`)
Tests specifically for the HTTP API service mode:
- Service lifecycle management
- API endpoint functionality
- Configuration management
- Error handling

### Integration Tests (`@pytest.mark.integration`)
Tests that verify integration between components:
- Full workflow testing
- Multi-step scenarios
- Real-world usage patterns

### E2E Tests (`@pytest.mark.e2e`)
Complete end-to-end scenarios:
- Service startup to query processing
- Polish language workflows
- Configuration persistence

## ðŸ“Š Test Results Example

```
ðŸŽ¯ Starting E2E Tests for NLP2CMD Service Mode
==================================================
ðŸ§ª Running test: Health Check
âœ… Health Check: PASSED

ðŸ§ª Running test: Service Info
âœ… Service Info: PASSED

ðŸ§ª Running test: Basic Query
âœ… Basic Query: PASSED

ðŸ§ª Running test: Polish Queries
âœ… Polish Queries: PASSED

ðŸ§ª Running test: Query with Explanation
âœ… Query with Explanation: PASSED

ðŸ§ª Running test: Config Management
âœ… Config Management: PASSED

ðŸ§ª Running test: Error Handling
âœ… Error Handling: PASSED

ðŸ§ª Running test: Concurrent Requests
âœ… Concurrent Requests: PASSED

==================================================
ðŸ“Š Test Results: 8/8 passed
ðŸŽ‰ All tests passed!
```

## ðŸ” Test Details

### Health Check Test
- **Endpoint**: `GET /health`
- **Expected**: `{"status": "healthy", "service": "nlp2cmd"}`
- **Purpose**: Verify service is running and responsive

### Service Info Test
- **Endpoint**: `GET /`
- **Expected**: Service info with configuration
- **Purpose**: Verify service metadata and config exposure

### Basic Query Test
- **Endpoint**: `POST /query`
- **Payload**: `{"query": "list files", "dsl": "shell"}`
- **Expected**: Successful command generation
- **Purpose**: Verify core NLP functionality

### Polish Queries Test
- **Queries**: Polish language inputs
- **Examples**: "znajdÅº pliki wiÄ™ksze niÅ¼ 100MB"
- **Purpose**: Verify Polish language support

### Configuration Management Test
- **Endpoints**: `GET /config`, `POST /config`
- **Purpose**: Verify runtime configuration updates

### Error Handling Test
- **Scenarios**: Invalid JSON, missing fields, bad data
- **Purpose**: Verify graceful error handling

### Concurrent Requests Test
- **Method**: 5 simultaneous requests
- **Purpose**: Verify concurrency handling

## ðŸ› ï¸ Test Implementation

### Service Manager Class
```python
class NLP2CMDServiceManager:
    """Manages NLP2CMD service lifecycle for testing."""
    
    def start_service(self) -> subprocess.Popen
    def stop_service(self)
    def wait_for_ready(self, timeout: int = 30) -> bool
    def cleanup(self)
```

### Test Fixtures
```python
@pytest.fixture(scope="function")
def service_manager():
    """Fixture for managing service lifecycle."""
    
@pytest.fixture(scope="function") 
def running_service(service_manager):
    """Fixture that provides a running service."""
```

## ðŸ”§ Configuration

### Test Environment
- **Host**: 127.0.0.1 (localhost)
- **Port**: 8001 (default, configurable)
- **Timeout**: 30 seconds startup
- **Environment**: Test .env file created automatically

### Pytest Configuration
```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
markers =
    e2e: End-to-end tests
    service: Service mode tests
    integration: Integration tests
```

## ðŸ“ Adding New Tests

### 1. Simple Test (using runner)
```python
def test_new_feature(self) -> bool:
    """Test new feature."""
    response = requests.get(f"{self.base_url}/new-endpoint")
    return response.status_code == 200
```

### 2. Pytest Test
```python
@pytest.mark.service
def test_new_feature(running_service):
    """Test new feature with pytest."""
    response = requests.get(f"{running_service.base_url}/new-endpoint")
    assert response.status_code == 200
```

### 3. Integration Test
```python
@pytest.mark.integration
def test_complete_workflow(running_service):
    """Test complete workflow."""
    # Step 1: Configure
    # Step 2: Process queries  
    # Step 3: Verify results
```

## ðŸš¨ Troubleshooting

### Service Won't Start
```bash
# Check dependencies
pip install fastapi uvicorn

# Check port availability
netstat -tlnp | grep :8001

# Enable debug mode
python3 run_e2e_tests.py --verbose
```

### Tests Fail
```bash
# Check service logs
python3 run_e2e_tests.py --verbose

# Run individual test
pytest tests/e2e/test_service_e2e.py::TestServiceE2E::test_health_check -v

# Check dependencies
pip install -r requirements.txt
```

### Port Conflicts
```bash
# Use different port
python3 run_e2e_tests.py --port 8002

# Or set environment variable
export NLP2CMD_TEST_PORT=8002
python3 run_e2e_tests.py
```

## ðŸ“ˆ Performance

### Test Duration
- **Service Startup**: ~2-5 seconds
- **All Tests**: ~15-20 seconds
- **Individual Tests**: ~0.5-2 seconds each

### Resource Usage
- **Memory**: ~50-100MB during tests
- **CPU**: Minimal during test execution
- **Network**: Localhost requests only

## ðŸŽ¯ Best Practices

1. **Isolation**: Each test gets a clean service instance
2. **Cleanup**: Automatic service shutdown after tests
3. **Timeouts**: Reasonable timeouts for all operations
4. **Error Messages**: Clear failure descriptions
5. **Idempotency**: Tests can be run multiple times
6. **Parallel Safe**: Tests don't interfere with each other

## ðŸ”„ CI/CD Integration

### GitHub Actions Example
```yaml
name: E2E Tests
on: [push, pull_request]
jobs:
  e2e:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run E2E tests
        run: python3 run_e2e_tests.py
```

### Docker Testing
```bash
# Build and test in Docker
docker build -t nlp2cmd-test .
docker run --rm nlp2cmd-test python3 run_e2e_tests.py
```

## ðŸ“š Related Documentation

- **[Service Mode Guide](../docs/SERVICE_MODE.md)** - Service mode documentation
- **[API Reference](../docs/api/README.md)** - API endpoint documentation
- **[Installation Guide](../INSTALLATION.md)** - Setup instructions
- **[Contributing Guide](../CONTRIBUTING.md)** - Development guidelines
